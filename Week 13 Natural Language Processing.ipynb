{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Week 13 Natural Language Processing.ipynb","provenance":[{"file_id":"1BdzuqHPpiqF0KKJGRUorYUgk4Na7mMcP","timestamp":1617584939279},{"file_id":"1kz23OWo8A_VgbDTHft7nPNKAyqDMIsNE","timestamp":1599664089485}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fEUqaxCDQA_j"},"source":["# Natural Language Processing & RNNs\n","\n","\n","\n","\n","\n","Proprietary material - Under Creative Commons 4.0 licence CC-BY-NC-ND https://creativecommons.org/licenses/by-nc-nd/4.0/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r_70dUvDwRZZ"},"source":["In the lecture, we learned about the use of Recurrent Neural Networks (RNNs) in NLP, the process of word embeddings and all the potential applications of NLP, a lot of which we presently use in our daily lives!"]},{"cell_type":"markdown","metadata":{"id":"fbrhY4k5UDWi"},"source":["There are a few contexts in which RNNs are generally applied:\n","\n","- Character-based: Predict the next character given a sequence of previous characters (e.g. it might predict \"e\" given \"Hello ther\").\n","\n","\n","- Word-based: Predict the next word given a sequence of previous words (e.g. it might predict (e.g. it might predict \"summer\" given \"School is out for\").\n","\n","- Learning word embeddings: Learn to convert a word into a numerical/vector representation."]},{"cell_type":"markdown","metadata":{"id":"oHOm7o9HWy9n"},"source":["In this tutorial we will mess around with word embeddings and then we'll train a character-based model!"]},{"cell_type":"markdown","metadata":{"id":"EtwxUTAnxrle"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"v3ZcGH_YySps"},"source":["We first need to get the standard imports to get this translation party started!"]},{"cell_type":"code","metadata":{"id":"Rd_kVPobx26S"},"source":["import numpy as np\n","import keras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"txsKvP1v2DVa"},"source":["# Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"CTjENKZP2M6p"},"source":["Word embeddings means that you take a normal word like 'hello' and converts it to a vector.\n","\n","To practice working with word embeddings, we will download the text8 corpus and train some word embeddings on it, then play around with the embeddings and see what we can discover.\n","\n","The text8 corpus is quite small compared to the corpora people generally use to train word embeddings, but in the interest of time, we will use text8. Our embeddings will be fairly noisy and unreliable as a result, though.\n","\n","gensim is a library that makes it super easy to train and work with word embeddings. All you need to do to train embeddings using Word2Vec is call Word2Vec(corpus)"]},{"cell_type":"code","metadata":{"id":"CtaLQOJ32NOm"},"source":["from gensim.models.word2vec import Word2Vec\n","from gensim import downloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MaUxDvCSlWH5","outputId":"86cb5ecc-6b10-4df0-8383-3f94b7ef3ea5"},"source":["# Download the text8 corpus (a fairly small corpus of text)\n","corpus = downloader.load('text8')\n","\n","# Train the Word2Vec model on the corpus\n","model = Word2Vec(corpus)\n","\n","# Save the vectors learned by the model in wv\n","wv = model.wv"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 31.6/31.6MB downloaded\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Dcy4LIdjqbfe"},"source":["This section is very open-ended. The main objective is for you to get comfortable working with continuous vector representations of words, so feel free to try out a variety of things!\n","\n","Here are some ideas:\n"," - Find the nearest neighbours of a few words, what patterns can you find?\n"," - Are some words semantically isolated? In other words, are words that are very different in meaning very 'far away' in space? Look for some words which are quite far away from other words in the embedding space.\n"," - Think of some analogy (i.e. A is to B as X is to ?) and find similar relationships|"]},{"cell_type":"code","metadata":{"id":"84q8E4tnl2Fp"},"source":["# See the vector for \"hello\"\n","print(wv['hello'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEp6Vcj7XGPA"},"source":["# Find the nearest neighbours of \"house\"\n","print(wv.most_similar('house', topn=10))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhSHs4QZQFXL"},"source":["# Find the nearest neighbours of \"silly\"\n","print(wv.most_similar('silly', topn=10))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wTMTkucTnVk-"},"source":["Now, play around with these word embeddings. You might want to look into \"most_similar\" function in particular. Documentation is [here](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py).\n","\n","This section is vert open-ended. Feel free to try whatever you'd like, but \n","Here are some ideas:\n","- Pick some analogy, such as \"Ottawa is to Canada as _ is to _\" and see what the model fills in\n","- Pick an odd word and look at the words that are the most similar\n","- Download various \n"]},{"cell_type":"code","metadata":{"id":"RlnMwDjRnUlZ"},"source":["# TODO - Get comfortable with word embeddings!\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rhwa27zU2ZcF"},"source":["# Setting up and training an LSTM\n","\n","A Long Short-Term Memory (LSTM) model is a specialized RNN which was built to have a better memory. Thankfully we can treat it the same as a regular RNN model on the surface - it just has unquestionably better performance.\n","\n","We will be training a character-based LSTM model to predict the next character given previous characters. We will train this model on the collected works of Friedrich Nietzsche.\n","\n","This basic idea of training a model that predicts future elements of a sequence given previous elements is very fundamental and underpins almost all models in NLP, such as [GPT-3](https://github.com/openai/gpt-3)."]},{"cell_type":"markdown","metadata":{"id":"D9xjbT-tvl9E"},"source":["In the interest of time we are working with a much smaller corpus than we would use in an industry application of NLP.\n","\n","First, we will just read the corpus and create dictionaries. We will then assign a numeric index to each character and create dictionaries mapping from characters to indices and vice versa."]},{"cell_type":"code","metadata":{"id":"UobJMeSM50SI"},"source":["from keras.utils.data_utils import get_file\n","\n","# Download the file from the internet\n","path = get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n","\n","# Read the file and make it all lowercase\n","with open(path, encoding='utf-8') as f:\n","    text = f.read().lower()\n","\n","# Print number of characters\n","print('corpus length:', len(text))\n","\n","# Get a sorted list of unique characters in text\n","chars = sorted(list(set(text)))\n","print('total chars:', len(chars))\n","\n","# Create dict for mapping characters to indices and vice versa\n","char_indices = dict((c, i) for i, c in enumerate(chars))\n","indices_char = dict((i, c) for i, c in enumerate(chars))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"plMRGCM-4AMj"},"source":["Next, we split the corpus into smaller sequences for training. Note that if you use these models in your own projects, you would want the number of sequences to be in the millions (this is currently in the hundreds of thousands)"]},{"cell_type":"code","metadata":{"id":"SC1eUP_vZ8HO"},"source":["# Cut the text into smaller sequences of <maxlen> characters\n","# shifting over the window <step> characters at a time.\n","maxlen = 40\n","step = 3\n","\n","# These will be our model's inputs after some preprocessing\n","sentences = []\n","\n","# These will be what our model will need to predict.\n","next_chars = []\n","\n","# Traverse the text to create the sequences above.\n","for i in range(0, len(text) - maxlen, step):\n","    sentences.append(text[i: i + maxlen])\n","    next_chars.append(text[i + maxlen])\n","print('# sequences:', len(sentences))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gOwHVAYL4bFU"},"source":["Next, we one-hot encode the characters to get a n\\*m\\*v shaped tensor (tensors are just higher dimensional matrices/vectors) for our training features x and n*v shaped tensor for our training labels y.\n","\n","Here, n is the number of sequences, m is the max length of a sequence (40 in our case) and v is the size of the vocabulary (57) in our case.\n"]},{"cell_type":"code","metadata":{"id":"Sh7Hyn4NZ-SI"},"source":["x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n","y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n","for i, sentence in enumerate(sentences):\n","    for t, char in enumerate(sentence):\n","      # Each row is the sentence's 40 characters where we have a one-hot\n","      # encoded vector for each character.\n","        x[i, t, char_indices[char]] = 1\n","    # Also one-hot encode the output variables\n","    y[i, char_indices[next_chars[i]]] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wp5JroK-5phz"},"source":["Great! We've prepared the data! Now, we have to define our model. Let's import the necessary functions"]},{"cell_type":"code","metadata":{"id":"trz9ytJxZ--C"},"source":["import random\n","import tensorflow as tf\n","#tf.enable_eager_execution()\n","from tensorflow.keras.callbacks import LambdaCallback\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, LSTM\n","from tensorflow.keras.optimizers import RMSprop"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hj-vSCyQ549P"},"source":["We will define a model with one LSTM layer with 128 neurons, followed by a softmax layer with one neuron per character. This second layer can be interpreted as the probabilities of each character coming next."]},{"cell_type":"code","metadata":{"id":"vCYl33rmaA4D"},"source":["model = Sequential([\n","    LSTM(128, input_shape=(maxlen, len(chars))),\n","    Dense(len(chars), activation='softmax'),\n","])\n","\n","# use the RMSprop optimizer with a learning rate of 0.01\n","optimizer = RMSprop(lr=0.01)\n","# compile the model using categorical cross-entropy loss\n","model.compile(loss='categorical_crossentropy', optimizer=optimizer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1a7LnpU8v1I"},"source":["Next, we define a function that prints some text output by our model based on the training it has gone through so far."]},{"cell_type":"code","metadata":{"id":"AxWBIbIYaCRY"},"source":["# Here we're just defining what should be done when an epoch ends.\n","# Don't worry if some of the syntax isn't super clear, its just for making\n","# the output nice and making some actual predictions. \n","def on_epoch_end(epoch, _):\n","  # Function invoked at end of each epoch. Prints generated text.\n","  print(f'[Epoch {epoch}]: ', end='')\n","\n","  # Select a random starting index and get maxlen characters from there\n","  start_index = random.randint(0, len(text) - maxlen - 1)\n","  sentence = text[start_index: start_index + maxlen]\n","  print(sentence, end='<END_SEED>')\n","  \n","  # Print 400 additional characters\n","  for i in range(400):\n","    # One-hot encode the input sentence\n","    x_pred = np.zeros((1, maxlen, len(chars)))\n","    for t, char in enumerate(sentence):\n","        x_pred[0, t, char_indices[char]] = 1.\n","    \n","    # Predict the next character based on the most probable char\n","    pred = np.argmax(model.predict(x_pred, verbose=0)[0])\n","    next_char = indices_char[pred]\n","\n","    # Add the currently predicted character to the sentence for our next prediction\n","    sentence = sentence[1:] + next_char\n","    print(next_char, end='')\n","  \n","print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ifbP92inIqdt"},"source":["Finally, let's fit the model to the data and see how it does!"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"a6dmD9N6aE7Q"},"source":["model.fit(x, y, batch_size=128, epochs=5, callbacks=[print_callback])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iynxhl5KJHfm"},"source":["Now, we can generate text using the fit model!"]},{"cell_type":"markdown","metadata":{"id":"pP0EZNJKmf9r"},"source":["**The importance of diversity**\n","\n","Our model returns probabilities for how likely each letter is to come next in the sequence, which is what we want! However, if we try to generate a sentence we'll usually get repeative text, for example, \"the stand the stand the stand...\" or \"of the strength of the strength...\", and this stems from the fact that we're always taking the most probable character. Then maybe we don't always want this.\n","\n","Diversity is how 'diverse' we want the text to be. If it's 0, it's like taking the most probable character (argmax). If it's 1, it's like taking a character essentially at random! Then, when generating a sentence, you have to play with the diversity value and find the right balance! "]},{"cell_type":"code","metadata":{"id":"sRy7ajfFli2Y"},"source":["# TODO - See what the outputs below look like when diversity is False\n","# TODO - Set using_diversity to True and play with the values of diversity\n","using_diversity = False\n","diversity = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVKUGh1Qld8a"},"source":["# Predict a single character\n","def sample_character():\n","  # One-hot encode the input sentence\n","  x_pred = np.zeros((1, maxlen, len(chars)))\n","  for t, char in enumerate(sentence):\n","      x_pred[0, t, char_indices[char]] = 1.\n","  \n","  if not using_diversity:\n","    pred = np.argmax(model.predict(x_pred, verbose=0)[0])\n","  # Pick one of the more likely characters (depending on diversity)\n","  else:\n","    pred = model.predict(x_pred, verbose=0)[0]\n","    pred = np.asarray(pred).astype('float64')\n","    pred = np.log(pred) / diversity\n","    exp_pred = np.exp(pred)\n","    pred = exp_pred / np.sum(exp_pred)\n","    probas = np.random.multinomial(1, pred, 1)\n","    pred = np.argmax(probas)\n","  \n","  return pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j37suxi3l0h0"},"source":["Predict a single letter (re-run for difference sentences):"]},{"cell_type":"code","metadata":{"id":"NYen9RUjJG0X"},"source":["# Select a random starting index and get maxlen characters from there\n","start_index = random.randint(0, len(text) - maxlen - 1)\n","sentence = text[start_index: start_index + maxlen]\n","print(sentence.replace(\"\\n\", \" \"), end=' --- Predicted: ')\n","\n","pred = sample_character()\n","next_char = indices_char[pred]\n","print(next_char, end='')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L7lJkv5Fl5vi"},"source":["Predict the rest of a sentence!"]},{"cell_type":"code","metadata":{"id":"6AKYRjg5l5LJ"},"source":["# Select a random starting index and get maxlen characters from there\n","start_index = random.randint(0, len(text) - maxlen - 1)\n","sentence = text[start_index: start_index + maxlen]\n","print(sentence.replace(\"\\n\", \" \"), end=' --- Predicted: ')\n","\n","# You can change the range of this loop to generate more or less characters\n","for i in range(80):\n","  pred = sample_character()\n","  next_char = indices_char[pred]\n","\n","  # Add the currently predicted character to the sentence for our next prediction\n","  sentence = sentence[1:] + next_char\n","  print(next_char, end='')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j1PUdtgmmHwl"},"source":["Is the sentence just repeating itself? Try switching 'using_diversity' to True and play with the diversity value (Between 0 and 1)!"]},{"cell_type":"markdown","metadata":{"id":"f73wameIIvYH"},"source":["## Further Experimentation\n","It's time to play with the model itself! Just remember that there's a cost to every change. Here's a few things you should try:\n","\n","Training Time vs Performance:\n","- Try using a subset of the total dataset for faster training (Smaller dataset = Faster training but worse performance).\n","- Try changing the learning rate (Larger learning rate = Faster training but worse performance, and vice versa)\n","\n","Complexity vs Performance:\n","- Change the model architecture by adding more units or layers.\n","- Change maxlen to use more or fewer characters. Changing the amount of context can greatly affect performance on generation tasks. However, larger maxlen means more inputs, which means a larger model and longer training time.\n","\n","We understand that training can take awhile, so you might want to consider doing some experimentation outside the designated tutorial time. You can just make a quick change and leave to do something else for a while. The ML training process involves a lot of that!"]},{"cell_type":"markdown","metadata":{"id":"pHzqAIyYkxfc"},"source":["#References\n","Lab developed for the LearnAI 2020-2021 academic year cohort (University of Toronto AI Club)"]}]}