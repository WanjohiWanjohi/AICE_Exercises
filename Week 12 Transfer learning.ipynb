{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fEUqaxCDQA_j"},"source":["# Transfer Learning\n","\n","Proprietary material - Under Creative Commons 4.0 licence CC-BY-NC-ND https://creativecommons.org/licenses/by-nc-nd/4.0/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_wN76PVZj7IX"},"source":["# Introduction\n","\n","This week we will cover the basics of how to apply Transfer Learning to a Neural Net in Keras."]},{"cell_type":"markdown","metadata":{"id":"Iv4sWqymQHCi"},"source":["Most, if not all the the big Deep Learning libraries come with some tools to apply Transfer Learning from another model. Some even include models trained and ready for deployment that can be used directly for Transfer Learning, and luckly for us, Keras includes that option."]},{"cell_type":"markdown","metadata":{"id":"-rfUA6QsnZHK"},"source":["## Layer Freezing"]},{"cell_type":"markdown","metadata":{"id":"p2KWyrtpchsG"},"source":["Before using Transfer Learning, its necessary to go over an intruduction on how to freeze a model layers and what does that mean."]},{"cell_type":"code","metadata":{"id":"IixFruqAy2KK"},"source":["# Importing relevant libraries\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.applications.vgg16 import VGG16"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"42AKtkAMgD1q"},"source":["First, lets load a Neural Net from the Keras library. \n","\n","The architecture is an Xception Net and trained in the ImageNet dataset.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GYL1yOe8P6KU","executionInfo":{"status":"ok","timestamp":1616972328442,"user_tz":180,"elapsed":8706,"user":{"displayName":"JOSÉ IGNACIO DÍAZ","photoUrl":"","userId":"14360442081919567546"}},"outputId":"195efa36-e70b-4e69-97c1-43dd87a91868"},"source":["vgg_net = VGG16(weights='imagenet', input_shape=(224, 224, 3), include_top=True) \n","vgg_net.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n","553467904/553467096 [==============================] - 5s 0us/step\n","Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 4096)              102764544 \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 4096)              16781312  \n","_________________________________________________________________\n","predictions (Dense)          (None, 1000)              4097000   \n","=================================================================\n","Total params: 138,357,544\n","Trainable params: 138,357,544\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QKqHjIjIzO-r"},"source":["A frozen layer in a model means that the layer weights wont be updated during training, so to apply transfer learning you have to control which layers need to be frozen or unfrozen before or in the middle of training."]},{"cell_type":"markdown","metadata":{"id":"jzAGmTXl5xp7"},"source":["To count the number of unfrozen or frozen weights, we can access the lists '*trainable\\_weights*' and  '*non\\_trainable\\_weights*' respectively.\n","\n","**Note:** Notice that the weights are 32, which is the double of the number of layers of the model (VGG16). This is because for each FC and Conv layer there is two weights (the kernel and bias)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hMDzr22Ewg8z","executionInfo":{"status":"ok","timestamp":1616972328444,"user_tz":180,"elapsed":6324,"user":{"displayName":"JOSÉ IGNACIO DÍAZ","photoUrl":"","userId":"14360442081919567546"}},"outputId":"008f78ce-8bef-4e9a-801f-bd33db538349"},"source":["print(\"Number of weights:\", len(vgg_net.weights))\n","print(\"Number of trainable weights:\", len(vgg_net.trainable_weights))\n","print(\"Number of frozen weights:\", len(vgg_net.non_trainable_weights))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of weights: 32\n","Number of trainable weights: 32\n","Number of frozen weights: 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"W6ZvS_y5_Axs"},"source":["We can freeze a layer by setting the layer '*trainable*' to False.\n","As a test, lets freeze the layers 'block1\\_conv2' and 'block4\\_conv3' in the positions 3 and 13 of the layers list."]},{"cell_type":"code","metadata":{"id":"50cUVENvyfDv"},"source":["vgg_net.layers[2].trainable = False\n","vgg_net.layers[13].trainable = False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FA9aVqIeAz2q"},"source":["Now lets check that those layers are frozzen."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsbM0pWU_r6X","executionInfo":{"status":"ok","timestamp":1616972340620,"user_tz":180,"elapsed":833,"user":{"displayName":"JOSÉ IGNACIO DÍAZ","photoUrl":"","userId":"14360442081919567546"}},"outputId":"87244761-9adc-4051-af99-266705b02069"},"source":["print(\"Number of weights:\", len(vgg_net.weights))\n","print(\"Number of trainable weights:\", len(vgg_net.trainable_weights))\n","print(\"Number of frozen weights:\", len(vgg_net.non_trainable_weights))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of weights: 32\n","Number of trainable weights: 28\n","Number of frozen weights: 4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GJeSGvnCEvI9"},"source":["Seting the trainable to False works on all the sublayers, so if we want to freeze all layers we can do this:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g1VCbu84A7L2","executionInfo":{"status":"ok","timestamp":1616972922930,"user_tz":180,"elapsed":802,"user":{"displayName":"JOSÉ IGNACIO DÍAZ","photoUrl":"","userId":"14360442081919567546"}},"outputId":"4ca5267d-069a-4645-df75-9de26567ff02"},"source":["vgg_net.trainable = False\n","\n","print(\"Number of weights:\", len(vgg_net.weights))\n","print(\"Number of trainable weights:\", len(vgg_net.trainable_weights))\n","print(\"Number of frozen weights:\", len(vgg_net.non_trainable_weights))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Number of weights: 32\n","Number of trainable weights: 0\n","Number of frozen weights: 32\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pZ5ex6NzGdkx"},"source":["The summary also indicates how many parametres can be trained in a model (at the bootom). "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JrbL00h5GLzc","executionInfo":{"status":"ok","timestamp":1616972981893,"user_tz":180,"elapsed":748,"user":{"displayName":"JOSÉ IGNACIO DÍAZ","photoUrl":"","userId":"14360442081919567546"}},"outputId":"7f7347c1-2abc-4e57-bd42-d79ff5245002"},"source":["vgg_net.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 25088)             0         \n","_________________________________________________________________\n","fc1 (Dense)                  (None, 4096)              102764544 \n","_________________________________________________________________\n","fc2 (Dense)                  (None, 4096)              16781312  \n","_________________________________________________________________\n","predictions (Dense)          (None, 1000)              4097000   \n","=================================================================\n","Total params: 138,357,544\n","Trainable params: 0\n","Non-trainable params: 138,357,544\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nE_WBWEeHDoJ"},"source":["# Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"r-DVJ4GXIg40"},"source":["Now lets finally prepare our model to train with Transfer Learning!\n","\n","First, lets grab one of the CNNs that come with Keras with no FC layers at the end.\n","\n","This can be done by passing the 'include\\_top' aprameter as False."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXFVTFo0GaNt","executionInfo":{"status":"ok","timestamp":1616986852563,"user_tz":180,"elapsed":1005,"user":{"displayName":"JOSÉ IGNACIO DÍAZ","photoUrl":"","userId":"14360442081919567546"}},"outputId":"aed09567-0998-4b9b-dea7-93e4b647bbe4"},"source":["vgg_net = VGG16(weights='imagenet', input_shape=(224, 224, 3), include_top=False) \n","vgg_net.trainable = False\n","vgg_net.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         [(None, 224, 224, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 0\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"00oy2eB-22Ce"},"source":["Now lets add a couple of FC layers on top. For now, lets asume our otput is a binary classifier for simplicity."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0m4hN7w61sbQ","executionInfo":{"status":"ok","timestamp":1616986860013,"user_tz":180,"elapsed":1651,"user":{"displayName":"JOSÉ IGNACIO DÍAZ","photoUrl":"","userId":"14360442081919567546"}},"outputId":"8928db7f-5b24-47c4-8501-25697b987c1f"},"source":["from keras.models import Sequential \n","from keras.layers import Dense, Flatten\n","from keras import optimizers\n","\n","model = Sequential()\n","\n","model.add(vgg_net)\n","model.add(Flatten())\n","model.add(Dense(4096, activation='relu'))\n","model.add(Dense(1024, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy',\n","              optimizer=optimizers.Adam(),\n","              metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"sequential_5\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n","_________________________________________________________________\n","flatten_5 (Flatten)          (None, 25088)             0         \n","_________________________________________________________________\n","dense_12 (Dense)             (None, 4096)              102764544 \n","_________________________________________________________________\n","dense_13 (Dense)             (None, 1024)              4195328   \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 1)                 1025      \n","=================================================================\n","Total params: 121,675,585\n","Trainable params: 106,960,897\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kZTR_0d67Z2D"},"source":["Now we have our model ready for the training loop!\n","\n","But lets leave that for the problem ;)\n","\n","Before geting to that, try to keep in mind the following considerations:\n","\n","\n","\n","1.   To accelerate the training, you can extract the features from the data using the frozen layers and train the rest of the model using those features.\n","\n","2.   The previos point can't be used if you are using data augmentation douring the training, but it can be done beforehand.\n","\n","3.  At the end of the training, its recomended to apply a bit of Fine Tunning to the model, by unfreezing the model and reducing the learning rate.\n","\n","4.  You can't add new layers before the frozen layers. If you need to, Fine Tuning is the only option.\n","\n","5. When using a CNN for transfer Learning, remember to modify your data so it has the expected size for the model.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VO09NjVnLOgF"},"source":["## Exercise \n","\n","Lets build a cat vs dog image classifier with a CNN. For that, you are free to so as you want, but we recomend at leats some of the following steps:\n","\n","1. Download the data from this link https://www.kaggle.com/c/dogs-vs-cats/data. If you want to use some other dataset, you are free to do so.\n","\n","2. Explore the data, visualize some examples and check the size of the images.\n","\n","3.  Choose an architecture from https://keras.io/api/applications/ to your liking. \n","\n","4. Reshape the images to the same size. For this you can use OpenCV like this:\n","\n","\n","\n","```\n","import cv2\n","import os\n","\n","path = 'path tho directory'\n","image_paths = os.listdir(path)\n","\n","for file in image_paths:\n","    image = cv2.imread(os.path.join(path, file))\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\timage = cv2.resize(image, (224, 224))\n","\n","    ...\n","```\n","\n","Then you can save the images to a list or data structure inside the loop. Some files include their class in the name, so this loop offers a great oportunity to prepare the labels too.\n","\n","\n","\n","4. If the shape of the data is diferent to the ImageNet data, you need to reshape the input. For this you can add the parameter 'input\\_tensor' when loading the trained model. For example, for an input of 128x128:\n","\n","\n","\n","```\n","vgg_net = VGG16(weights=\"imagenet\", include_top=False,\n","\tinput_tensor=Input(shape=(128, 128, 3)))\n","```\n","\n","5. Freeze the model and add some new FC and Dropout layers with apropiate sizes. \n","\n","5. Separate the data into train and validation by 80% train and 20% validation. \n","\n","6. Train the model over the train data. Use the Adam optimizer and choose a loss apropiate to your problem. As an example, in case its a binary classifier use  Binary Cross Entropy.\n","\n","7. Optional: Unfreeze the model at the end and train once again the model using Fine Tuning.\n","\n","8. Analize the training time and the performance of the model in the test data. Optional: Share some of you results with the team and discuss about the architecture, loss and performace of your model. \n","\n","\n","We understand that some of this steps might be a bit complicated or unclear for some, so dont be afraid to ask questions to the team in the public channels or check the references for some guidance.  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"CiTN8qnpKqxV"},"source":["## References\n","\n","\n","The oficial Keras Tutorial\n","\n","https://keras.io/guides/transfer_learning/\n","\n","List and examples of some architectures in Keras \n","\n","https://keras.io/api/applications/\n","\n","A great tutorial that goes deeper into Transfer Learning, but we recomend checking it out after being done with the exercise. And I trust you will.\n","\n","https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a"]}]}